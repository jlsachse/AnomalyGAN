{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CWRU Experiments\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lib.transformers as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from lib.model import Ganomaly, GanomalyNet\n",
    "from lib.visualization import GANomalyBoard, rename_tensorboard_key\n",
    "\n",
    "from skorch.callbacks import PassthroughScoring\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwruData0 = pd.read_parquet('data/cwru0.parquet')\n",
    "cwruData1 = pd.read_parquet('data/cwru1.parquet')\n",
    "\n",
    "cwruData = pd.concat([cwruData0, cwruData1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100  # size of the latent z vector\n",
    "ngf = 64  # units of generator\n",
    "ndf = 64  # units of discriminator\n",
    "nc = 1  # number of channels\n",
    "batch_size = 32\n",
    "lr = 0.0002\n",
    "beta1 = 0.5  # for adam\n",
    "max_epochs = 5\n",
    "ngpu = 1\n",
    "isize = 32  # 32 is easier than 28 to work with\n",
    "workers = 2  # for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganomaly = GanomalyNet(\n",
    "    Ganomaly,\n",
    "    module__isize = isize,\n",
    "    module__nz=nz,\n",
    "    module__ndf=ndf,\n",
    "    module__ngf=ngf,\n",
    "    module__nc=nc,\n",
    "    module__ngpu=ngpu,\n",
    "    \n",
    "    module__w_lat = 1,\n",
    "    \n",
    "    criterion=torch.nn.BCELoss,\n",
    "\n",
    "    optimizer_gen=torch.optim.Adam,\n",
    "    optimizer_gen__lr=0.0002,\n",
    "    optimizer_gen__betas=(beta1, 0.999),\n",
    "\n",
    "    optimizer_dis=torch.optim.Adam,\n",
    "    optimizer_dis__lr=0.00002,\n",
    "    optimizer_dis__betas=(beta1, 0.999),\n",
    "\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=100,\n",
    "\n",
    "    train_split=False,  # not implemented\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__num_workers=workers,\n",
    "    iterator_valid__num_workers=workers,\n",
    "\n",
    "    callbacks=[\n",
    "        PassthroughScoring('loss_dis', on_train=True),\n",
    "        PassthroughScoring('loss_gen', on_train=True),\n",
    "        PassthroughScoring('loss_gen_fra', on_train=True),\n",
    "        PassthroughScoring('loss_gen_app', on_train=True),\n",
    "        PassthroughScoring('loss_gen_lat', on_train=True)  \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Building a pipeline of custom transformers to fetch and preprocess CWRU data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlsachse\\Documents\\Bachelorarbeit\\Implementation\\ganomaly\\lib\\transformers.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X_ = np.array(X_)\n"
     ]
    }
   ],
   "source": [
    "image_size = 56\n",
    "\n",
    "selection_pipeline = \\\n",
    "Pipeline(steps=[\n",
    "                ('DataSelector', tf.DataSelector(columns = ['fanEndData', 'driveEndData'], column_values = {'condition': ['Normal Baseline'], 'sampleRate': [12000]})),\n",
    "                ('ArrayFlattener', tf.ArrayFlattener()),\n",
    "                #('ArrayEqualizer', tf.ArrayEqualizer()),\n",
    "                ('ArrayChunker', tf.ArrayChunker(image_size**2)),\n",
    "                ('ArrayFlattener2', tf.ArrayFlattener()),\n",
    "                ('ArrayReshaper', tf.ArrayReshaper((1, image_size, image_size)))\n",
    "               ])\n",
    "\n",
    "chunked_normal_data = selection_pipeline.transform(cwruData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test-split of the normal CWRU data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(chunked_normal_data, train_size = 400, test_size= 451, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_parameters = {\n",
    "    'module_w_fra': list(range(0, 101, 10)),\n",
    "    'module_w_app': list(range(0, 101, 10)),\n",
    "    'module_w_lat': list(range(0, 101, 10)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganomaly_gs = GridSearchCV(ganomaly, search_parameters, refit=False, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ganomaly_gs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization (best parameters)\n",
    "Adding a TensorBoard for the visualization of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<skorch.callbacks.scoring.PassthroughScoring at 0x2cdf23bd148>,\n",
       " <skorch.callbacks.scoring.PassthroughScoring at 0x2cdf23bd188>,\n",
       " <skorch.callbacks.scoring.PassthroughScoring at 0x2cdf23bd1c8>,\n",
       " <skorch.callbacks.scoring.PassthroughScoring at 0x2cdf23bd208>,\n",
       " <skorch.callbacks.scoring.PassthroughScoring at 0x2cdf23bd248>,\n",
       " <lib.visualization.GANomalyBoard at 0x2cdf22ddd48>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ganomaly_board = GANomalyBoard(SummaryWriter(), key_mapper = rename_tensorboard_key, close_after_train = False)\n",
    "ganomaly.callbacks += [ganomaly_board]\n",
    "\n",
    "ganomaly.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_dis    loss_gen    loss_gen_app    loss_gen_fra    loss_gen_lat    train_loss      dur\n",
      "-------  ----------  ----------  --------------  --------------  --------------  ------------  -------\n",
      "      1      0.6467      1.7578          0.1721          0.7455          0.8402        2.4046  15.3742\n",
      "      2      0.6364      0.9271          0.1298          0.5776          0.2197        1.5636  14.7292\n",
      "      3      0.6221      0.6477          0.0873          0.4490          0.1114        1.2698  16.4930\n",
      "      4      0.6063      0.4939          0.0730          0.3510          0.0699        1.1002  16.5689\n",
      "      5      0.5897      0.3925          0.0668          0.2766          0.0491        0.9822  16.6203\n",
      "      6      0.5723      0.3217          0.0639          0.2202          0.0377        0.8940  18.3905\n",
      "      7      0.5537      0.2714          0.0626          0.1772          0.0316        0.8251  17.5113\n",
      "      8      0.5326      0.2335          0.0611          0.1443          0.0281        0.7661  14.5818\n",
      "      9      0.5096      0.2017          0.0600          0.1189          0.0228        0.7113  14.5729\n",
      "     10      0.4843      0.1804          0.0593          0.0995          0.0216        0.6647  14.3662\n",
      "     11      0.4586      0.1627          0.0589          0.0839          0.0198        0.6212  16.7069\n",
      "     12      0.4297      0.1484          0.0585          0.0717          0.0182        0.5781  16.1401\n",
      "     13      0.3996      0.1368          0.0582          0.0619          0.0168        0.5364  16.1473\n",
      "     14      0.3720      0.1287          0.0580          0.0539          0.0168        0.5007  14.6541\n",
      "     15      0.3441      0.1208          0.0576          0.0475          0.0157        0.4649  14.7711\n",
      "     16      0.3176      0.1122          0.0573          0.0421          0.0128        0.4298  14.7612\n",
      "     17      0.2943      0.1086          0.0571          0.0376          0.0140        0.4030  14.7584\n",
      "     18      0.2734      0.1046          0.0570          0.0339          0.0137        0.3780  15.2837\n",
      "     19      0.2549      0.1009          0.0568          0.0307          0.0133        0.3558  14.8320\n",
      "     20      0.2367      0.0957          0.0565          0.0279          0.0113        0.3324  14.5817\n",
      "     21      0.2238      0.0950          0.0564          0.0256          0.0130        0.3188  17.4357\n",
      "     22      0.2097      0.0911          0.0562          0.0237          0.0111        0.3008  21.6791\n",
      "     23      0.1984      0.0909          0.0562          0.0218          0.0128        0.2892  20.6336\n",
      "     24      0.1883      0.0874          0.0560          0.0203          0.0112        0.2757  17.8308\n",
      "     25      0.1783      0.0855          0.0558          0.0189          0.0109        0.2639  18.8494\n",
      "     26      0.1696      0.0832          0.0556          0.0176          0.0099        0.2528  20.2388\n",
      "     27      0.1620      0.0829          0.0555          0.0166          0.0108        0.2449  19.4531\n",
      "     28      0.1559      0.0815          0.0554          0.0156          0.0105        0.2374  22.7365\n",
      "     29      0.1502      0.0806          0.0552          0.0146          0.0108        0.2308  20.8216\n",
      "     30      0.1443      0.0787          0.0550          0.0139          0.0098        0.2231  21.6449\n",
      "     31      0.1394      0.0781          0.0548          0.0131          0.0101        0.2175  20.6301\n",
      "     32      0.1336      0.0770          0.0547          0.0124          0.0098        0.2106  20.5595\n",
      "     33      0.1297      0.0762          0.0545          0.0117          0.0099        0.2058  20.0050\n",
      "     34      0.1250      0.0761          0.0545          0.0112          0.0104        0.2011  20.4075\n",
      "     35      0.1210      0.0749          0.0543          0.0108          0.0098        0.1959  20.4571\n",
      "     36      0.1168      0.0749          0.0542          0.0102          0.0105        0.1918  20.9320\n",
      "     37      0.1131      0.0736          0.0540          0.0098          0.0097        0.1867  20.9117\n",
      "     38      0.1092      0.0721          0.0538          0.0094          0.0089        0.1813  22.4181\n",
      "     39      0.1062      0.0725          0.0537          0.0090          0.0098        0.1786  21.0759\n",
      "     40      0.1028      0.0708          0.0535          0.0087          0.0086        0.1736  20.5175\n",
      "     41      0.0996      0.0713          0.0534          0.0084          0.0095        0.1709  22.2605\n",
      "     42      0.0968      0.0690          0.0532          0.0080          0.0077        0.1659  22.1681\n",
      "     43      0.0946      0.0706          0.0531          0.0078          0.0097        0.1652  21.7982\n",
      "     44      0.0915      0.0697          0.0530          0.0075          0.0091        0.1612  20.3553\n",
      "     45      0.0892      0.0691          0.0529          0.0072          0.0090        0.1583  19.3686\n",
      "     46      0.0869      0.0683          0.0528          0.0071          0.0085        0.1551  20.7780\n",
      "     47      0.0847      0.0688          0.0527          0.0068          0.0093        0.1535  20.1421\n",
      "     48      0.0826      0.0681          0.0526          0.0066          0.0089        0.1507  21.0785\n",
      "     49      0.0811      0.0680          0.0525          0.0064          0.0091        0.1492  22.5775\n",
      "     50      0.0792      0.0672          0.0524          0.0062          0.0086        0.1464  21.0615\n",
      "     51      0.0772      0.0665          0.0523          0.0060          0.0082        0.1437  20.6418\n",
      "     52      0.0755      0.0662          0.0522          0.0059          0.0081        0.1416  22.6830\n",
      "     53      0.0744      0.0665          0.0521          0.0057          0.0087        0.1409  20.3935\n",
      "     54      0.0727      0.0651          0.0519          0.0055          0.0076        0.1377  19.5014\n",
      "     55      0.0711      0.0654          0.0519          0.0054          0.0081        0.1365  21.2114\n",
      "     56      0.0699      0.0646          0.0518          0.0052          0.0076        0.1346  22.1547\n",
      "     57      0.0688      0.0653          0.0517          0.0051          0.0085        0.1341  21.2243\n",
      "     58      0.0673      0.0646          0.0517          0.0049          0.0080        0.1319  20.1657\n",
      "     59      0.0660      0.0632          0.0515          0.0048          0.0068        0.1292  21.3740\n",
      "     60      0.0663      0.0659          0.0517          0.0047          0.0096        0.1322  20.7278\n",
      "     61      0.0657      0.0642          0.0514          0.0045          0.0082        0.1299  21.0118\n",
      "     62      0.0641      0.0646          0.0514          0.0044          0.0088        0.1286  19.6249\n",
      "     63      0.0641      0.0630          0.0513          0.0043          0.0074        0.1271  20.8684\n",
      "     64      0.0657      0.0631          0.0512          0.0042          0.0078        0.1288  19.1462\n",
      "     65      0.0612      0.0619          0.0510          0.0041          0.0068        0.1230  19.0062\n",
      "     66      0.0613      0.0629          0.0509          0.0039          0.0081        0.1242  19.8703\n",
      "     67      0.0650      0.0618          0.0508          0.0038          0.0072        0.1269  20.1471\n",
      "     68      0.0638      0.0617          0.0506          0.0037          0.0074        0.1255  20.5526\n",
      "     69      0.0587      0.0613          0.0504          0.0036          0.0073        0.1200  20.3937\n",
      "     70      0.0669      0.0620          0.0503          0.0035          0.0081        0.1289  20.6445\n",
      "     71      0.0598      0.0609          0.0500          0.0034          0.0075        0.1207  20.7448\n",
      "     72      0.0576      0.0594          0.0497          0.0033          0.0064        0.1170  19.7712\n",
      "     73      0.0612      0.0609          0.0497          0.0032          0.0080        0.1220  19.8525\n",
      "     74      0.0632      0.0616          0.0496          0.0031          0.0089        0.1249  20.8204\n",
      "     75      0.0675      0.0603          0.0496          0.0030          0.0077        0.1279  19.4333\n",
      "     76      0.0577      0.0578          0.0492          0.0029          0.0057        0.1155  21.6409\n",
      "     77      0.0551      0.0592          0.0490          0.0029          0.0073        0.1142  20.7521\n",
      "     78      0.0586      0.0603          0.0491          0.0028          0.0084        0.1189  20.8163\n",
      "     79      0.0568      0.0600          0.0490          0.0027          0.0083        0.1168  20.3179\n",
      "     80      0.0572      0.0575          0.0488          0.0027          0.0060        0.1147  20.5598\n",
      "     81      0.0526      0.0573          0.0486          0.0026          0.0061        0.1100  20.6572\n",
      "     82      0.0548      0.0583          0.0486          0.0026          0.0071        0.1131  20.4239\n",
      "     83      0.0540      0.0593          0.0486          0.0025          0.0082        0.1133  20.6216\n",
      "     84      0.0554      0.0574          0.0484          0.0025          0.0065        0.1129  21.4093\n",
      "     85      0.0506      0.0573          0.0483          0.0024          0.0065        0.1079  22.3529\n",
      "     86      0.0497      0.0574          0.0484          0.0024          0.0066        0.1071  20.9342\n",
      "     87      0.0568      0.0579          0.0483          0.0023          0.0073        0.1147  21.0372\n",
      "     88      0.0507      0.0569          0.0483          0.0023          0.0063        0.1076  19.6555\n",
      "     89      0.0489      0.0582          0.0482          0.0022          0.0078        0.1071  19.4901\n",
      "     90      0.0519      0.0577          0.0481          0.0022          0.0074        0.1096  19.3550\n",
      "     91      0.0474      0.0571          0.0482          0.0022          0.0067        0.1045  19.5828\n",
      "     92      0.0505      0.0562          0.0480          0.0021          0.0061        0.1067  19.5545\n",
      "     93      0.0501      0.0563          0.0481          0.0021          0.0061        0.1064  19.5962\n",
      "     94      0.0457      0.0565          0.0478          0.0021          0.0066        0.1021  21.1714\n",
      "     95      0.0596      0.0599          0.0489          0.0020          0.0090        0.1194  20.0790\n",
      "     96      0.0494      0.0553          0.0480          0.0020          0.0053        0.1047  20.5251\n",
      "     97      0.0432      0.0551          0.0477          0.0019          0.0054        0.0983  20.9681\n",
      "     98      0.0439      0.0561          0.0478          0.0019          0.0064        0.1000  20.6537\n",
      "     99      0.0447      0.0560          0.0477          0.0019          0.0064        0.1007  20.0478\n",
      "    100      0.0477      0.0578          0.0478          0.0019          0.0082        0.1054  20.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'lib.model.GanomalyNet'>[initialized](\n",
       "  module_=Ganomaly(\n",
       "    (l_fra): BCELoss()\n",
       "    (l_app): L1Loss()\n",
       "    (l_dis): L1Loss()\n",
       "    (discriminator): NetD(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Conv2d(256, 100, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "        (Sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (generator): NetG(\n",
       "      (encoder1): Encoder(\n",
       "        (main): Sequential(\n",
       "          (initial-conv-1-64): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (initial-relu-64): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (pyramid-64-128-conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (pyramid-128-batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pyramid-128-relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (pyramid-128-256-conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (pyramid-256-batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pyramid-256-relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (final-256-1-conv): Conv2d(256, 100, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (main): Sequential(\n",
       "          (initial-100-256-convt): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "          (initial-256-batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (initial-256-relu): ReLU(inplace=True)\n",
       "          (pyramid-256-128-convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (pyramid-128-batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pyramid-128-relu): ReLU(inplace=True)\n",
       "          (pyramid-128-64-convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (pyramid-64-batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pyramid-64-relu): ReLU(inplace=True)\n",
       "          (final-64-1-convt): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (final-1-tanh): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (encoder2): Encoder(\n",
       "        (main): Sequential(\n",
       "          (initial-conv-1-64): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (initial-relu-64): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (pyramid-64-128-conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (pyramid-128-batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pyramid-128-relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (pyramid-128-256-conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (pyramid-256-batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pyramid-256-relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (final-256-1-conv): Conv2d(256, 100, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ganomaly.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028867852"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ganomaly.predict(test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1594355 , 0.1406145 , 0.14918292, 0.14804487, 0.14178324,\n",
       "       0.14477463, 0.13752337, 0.14540938, 0.15198371, 0.14790529,\n",
       "       0.15870446, 0.16107456, 0.1543032 , 0.14570117, 0.16333222,\n",
       "       0.15785691, 0.15391228, 0.14082545, 0.15375912, 0.15185909,\n",
       "       0.14997056, 0.1548337 , 0.15414204, 0.15421586, 0.14926204,\n",
       "       0.15465142, 0.15416095, 0.16461125, 0.15822984, 0.15712994,\n",
       "       0.15597245, 0.15309568, 0.14750645, 0.13814154, 0.15503491,\n",
       "       0.15386963, 0.15916605, 0.16225797, 0.41035908, 0.3862005 ,\n",
       "       0.42294383, 0.4652704 , 0.39976633, 0.33235806, 0.40886942,\n",
       "       0.42219275, 0.37192416, 0.35318452, 0.4393332 , 0.47563294,\n",
       "       0.38049513, 0.38787317, 0.44683915, 0.45800355, 0.36684597,\n",
       "       0.38384235, 0.4708737 , 0.43070483, 0.35233247, 0.38967684,\n",
       "       0.4623214 , 0.4322304 , 0.3431528 , 0.48618048, 0.5155954 ,\n",
       "       0.45486328, 0.3678665 , 0.47173417, 0.42963988, 0.35994455,\n",
       "       0.3420385 , 0.44578534, 0.40666974, 0.33889282, 0.38821733,\n",
       "       0.4608119 , 0.19952236, 0.18165761, 0.18532631, 0.18397911,\n",
       "       0.19265977, 0.18354762, 0.18395649, 0.17968276, 0.18746212,\n",
       "       0.19532546, 0.1790368 , 0.17915203, 0.18696895, 0.20095226,\n",
       "       0.19345045, 0.1848903 , 0.18994677, 0.19438484, 0.2007564 ,\n",
       "       0.1894513 , 0.19014655, 0.19516623, 0.21236265, 0.19466802,\n",
       "       0.17998715, 0.2034463 , 0.19891553, 0.20011723, 0.18304251,\n",
       "       0.20075426, 0.18299511, 0.19508928, 0.19743145, 0.1863387 ,\n",
       "       0.18413222, 0.18068607, 0.20268925, 0.17475015, 0.17720032,\n",
       "       0.5213386 , 0.44538203, 0.47239733, 0.4284156 , 0.45277584,\n",
       "       0.43270743, 0.4181118 , 0.4852013 , 0.41206086, 0.50532556,\n",
       "       0.39534968, 0.51593256, 0.43099427, 0.49741846, 0.45940772,\n",
       "       0.46203238, 0.5139115 , 0.44598082, 0.550362  , 0.43970042,\n",
       "       0.5340151 , 0.42840037, 0.51584756, 0.46623284, 0.47057158,\n",
       "       0.54528093, 0.4500476 , 0.5109538 , 0.43151388, 0.5207373 ,\n",
       "       0.49361446, 0.45504212, 0.5005732 , 0.4178345 , 0.5299287 ,\n",
       "       0.39998263, 0.51729816, 0.40972042, 0.44561815, 0.14271301,\n",
       "       0.12723142, 0.13803922, 0.134087  , 0.14427978, 0.14456634,\n",
       "       0.13812962, 0.14133805, 0.1433638 , 0.14759298, 0.14372945,\n",
       "       0.15487203, 0.14148046, 0.13928694, 0.14959088, 0.1441175 ,\n",
       "       0.14700577, 0.14951678, 0.15621588, 0.14708154, 0.14354327,\n",
       "       0.15721911, 0.15418737, 0.14867453, 0.1573784 , 0.1394565 ,\n",
       "       0.14603558, 0.13958415, 0.14197132, 0.14675584, 0.14125939,\n",
       "       0.15690333, 0.14588398, 0.1503765 , 0.14366752, 0.15377524,\n",
       "       0.1621916 , 0.1532396 , 0.42707768, 0.4130851 , 0.4281442 ,\n",
       "       0.4208836 , 0.44157177, 0.4171878 , 0.4063325 , 0.45629048],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ganomaly.predict(selection_pipeline.set_params(DataSelector__column_values = {'condition': ['Outer Race Fault'], 'sampleRate': [12000]}).transform(cwruData)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
