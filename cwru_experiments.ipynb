{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CWRU Experiments\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lib.transformers as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from lib.models import Ganomaly1d, Ganomaly2d, GanomalyFE, GanomalyNet\n",
    "from lib.visualization import GANomalyBoard, rename_tensorboard_key\n",
    "\n",
    "from skorch.callbacks import PassthroughScoring\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import tensorflow\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwruData0 = pd.read_parquet('data/cwru0.parquet')\n",
    "cwruData1 = pd.read_parquet('data/cwru1.parquet')\n",
    "\n",
    "cwruData = pd.concat([cwruData0, cwruData1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b4905cc9b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, isize, max_epochs, directory, needs_feature_engineering = False, ngpu = 0, nz = 100, ndf = 64, ngf = 64, nc = 1, batch_size = 32, lr = 0.0002, beta1 = 0.5, beta2 = 0.999, workers = 2):\n",
    "    \n",
    "    if not needs_feature_engineering:\n",
    "        output_model = GanomalyNet(\n",
    "            model,\n",
    "            module__isize = isize,\n",
    "            module__nz=nz,\n",
    "            module__ndf=ndf,\n",
    "            module__ngf=ngf,\n",
    "            module__nc=nc,\n",
    "            module__ngpu=ngpu,\n",
    "\n",
    "            device = torch.device(\"cuda:0\") if torch.cuda.is_available() else 'cpu',\n",
    "\n",
    "            criterion=torch.nn.BCELoss,\n",
    "\n",
    "            optimizer_gen=torch.optim.Adam,\n",
    "            optimizer_gen__lr=lr,\n",
    "            optimizer_gen__betas=(beta1, beta2),\n",
    "\n",
    "            optimizer_dis=torch.optim.Adam,\n",
    "            optimizer_dis__lr=lr,\n",
    "            optimizer_dis__betas=(beta1, beta2),\n",
    "\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=max_epochs,\n",
    "\n",
    "            train_split=False,  # not implemented\n",
    "            iterator_train__shuffle=True,\n",
    "            iterator_train__num_workers=workers,\n",
    "            iterator_valid__num_workers=workers,\n",
    "\n",
    "            callbacks=[\n",
    "                PassthroughScoring('loss_dis', on_train=True),\n",
    "                PassthroughScoring('loss_gen', on_train=True),\n",
    "                PassthroughScoring('loss_gen_fra', on_train=True),\n",
    "                PassthroughScoring('loss_gen_app', on_train=True),\n",
    "                PassthroughScoring('loss_gen_lat', on_train=True),\n",
    "                GANomalyBoard(SummaryWriter(log_dir= 'runs/' + directory), key_mapper = rename_tensorboard_key, close_after_train = False)\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "            output_model = GanomalyNet(\n",
    "            model,\n",
    "            module__isize = isize,\n",
    "            module__ngpu=ngpu,\n",
    "\n",
    "            device = torch.device(\"cuda:0\") if torch.cuda.is_available() else 'cpu',\n",
    "\n",
    "            criterion=torch.nn.BCELoss,\n",
    "\n",
    "            optimizer_gen=torch.optim.Adam,\n",
    "            optimizer_gen__lr=lr,\n",
    "            optimizer_gen__betas=(beta1, beta2),\n",
    "\n",
    "            optimizer_dis=torch.optim.Adam,\n",
    "            optimizer_dis__lr=lr,\n",
    "            optimizer_dis__betas=(beta1, beta2),\n",
    "\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=max_epochs,\n",
    "\n",
    "            train_split=False,  # not implemented\n",
    "            iterator_train__shuffle=True,\n",
    "            iterator_train__num_workers=workers,\n",
    "            iterator_valid__num_workers=workers,\n",
    "\n",
    "            callbacks=[\n",
    "                PassthroughScoring('loss_dis', on_train=True),\n",
    "                PassthroughScoring('loss_gen', on_train=True),\n",
    "                PassthroughScoring('loss_gen_fra', on_train=True),\n",
    "                PassthroughScoring('loss_gen_app', on_train=True),\n",
    "                PassthroughScoring('loss_gen_lat', on_train=True),\n",
    "                GANomalyBoard(SummaryWriter(log_dir= 'runs/' + directory), key_mapper = rename_tensorboard_key, close_after_train = False)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    return output_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Building a pipeline of custom transformers to fetch and preprocess CWRU data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_preprocessing = \\\n",
    "Pipeline(steps=[\n",
    "                ('DataSelector', tf.DataSelector(columns = ['fanEndData', 'driveEndData'], column_values = {'condition': ['Normal Baseline'], 'sampleRate': [12000]})),\n",
    "                ('ArrayFlattener', tf.ArrayFlattener()),\n",
    "                ('ArrayChunker', tf.ArrayChunker(3136)),\n",
    "                ('ArrayFlattener2', tf.ArrayFlattener())\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlsachse\\Documents\\Bachelorarbeit\\Implementation\\ganomaly\\lib\\transformers.py:90: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X_ = np.array(X_)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(common_preprocessing.transform(cwruData), train_size = 400, test_size= 400, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_dis    loss_gen    loss_gen_app    loss_gen_fra    loss_gen_lat    train_loss      dur\n",
      "-------  ----------  ----------  --------------  --------------  --------------  ------------  -------\n",
      "      1      1.0411      4.7213          0.8314          0.8944          2.9954        5.7624  32.2493\n",
      "      2      1.0144      3.4045          0.8322          0.6845          1.8879        4.4189  32.5450\n",
      "  epoch    loss_dis    loss_gen    loss_gen_app    loss_gen_fra    loss_gen_lat    train_loss      dur\n",
      "-------  ----------  ----------  --------------  --------------  --------------  ------------  -------\n",
      "      1      1.0075      4.3600          0.8300          0.5880          2.9420        5.3675  26.2801\n",
      "      2      0.7574      2.5714          0.8326          0.0299          1.7089        3.3289  25.4524\n",
      "  epoch    loss_dis    loss_gen    loss_gen_app    loss_gen_fra    loss_gen_lat    train_loss     dur\n",
      "-------  ----------  ----------  --------------  --------------  --------------  ------------  ------\n",
      "      1      0.7473      1.5733          0.7726          0.7960          0.0047        2.3207  5.2218\n",
      "      2      0.7082      1.5665          0.7726          0.7910          0.0029        2.2747  4.5446\n",
      "  epoch    loss_dis    loss_gen    loss_gen_app    loss_gen_fra    loss_gen_lat    train_loss      dur\n",
      "-------  ----------  ----------  --------------  --------------  --------------  ------------  -------\n",
      "      1      0.9961      4.2468          0.8234          0.4563          2.9671        5.2429  26.2419\n",
      "      2      0.7318      2.5142          0.8258          0.0191          1.6694        3.2460  26.6858\n",
      "  epoch    loss_dis    loss_gen    loss_gen_app    loss_gen_fra    loss_gen_lat    train_loss      dur\n",
      "-------  ----------  ----------  --------------  --------------  --------------  ------------  -------\n",
      "      1      1.0390      2.9261          0.8098          0.6803          1.4360        3.9651  10.2688\n",
      "      2      0.9804      1.6430          0.8100          0.2559          0.5771        2.6234  10.1999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('ArrayFFT', ArrayFFT()),\n",
       "                ('StandardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('ArrayReshaper2', ArrayReshaper(shape=(1, 1568))),\n",
       "                ('Model',\n",
       "                 <class 'lib.models.GanomalyNet'>[initialized](\n",
       "  module_=Ganomaly1d(\n",
       "    (l_fra): BCELoss()\n",
       "    (l_app): L1Loss()\n",
       "    (l_dis): L1Loss()\n",
       "    (discriminator): DiscriminatorNet1d(\n",
       "      (features): Sequent...\n",
       "          (pyramid-128-batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pyramid-128-256-convt): Conv1d(128, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
       "          (pyramid-relu-256): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (pyramid-256-batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (final-256-1-convt): Conv1d(256, 100, kernel_size=(9,), stride=(1,), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_epochs = 2\n",
    "\n",
    "ganomaly_timeseries = build_model(Ganomaly1d, 3136, max_epochs, 'timeseries')\n",
    "ganomaly_stacked_ts = build_model(Ganomaly2d, 56, max_epochs, 'stacked_timeseries')\n",
    "ganomaly_fft = build_model(Ganomaly1d, 1568, max_epochs, 'fourier_transform')\n",
    "ganomaly_stft = build_model(Ganomaly2d, 56, max_epochs, 'short_term_fouriers')\n",
    "ganomaly_fe = build_model(GanomalyFE, 4, max_epochs, 'feature_extraction', True)\n",
    "\n",
    "model_timeseries = \\\n",
    "Pipeline(steps=[\n",
    "                ('StandardScaler', StandardScaler()),\n",
    "                ('ArrayReshaper', tf.ArrayReshaper((1, 3136))),\n",
    "                ('Model', ganomaly_timeseries)\n",
    "               ])\n",
    "\n",
    "model_stacked_ts = \\\n",
    "Pipeline(steps=[\n",
    "                ('StandardScaler', StandardScaler()),\n",
    "                ('ArrayReshaper', tf.ArrayReshaper((1, 56, 56))),\n",
    "                ('Model', ganomaly_stacked_ts)\n",
    "               ])\n",
    "\n",
    "model_fe = \\\n",
    "Pipeline(steps=[\n",
    "                ('FeatureExtractor', tf.FeatureExtractor(axis = 1)),\n",
    "                ('StandardScaler', StandardScaler()),\n",
    "                ('ArrayReshaper', tf.ArrayReshaper((1, 4, 4))),\n",
    "                ('Model', ganomaly_fe)\n",
    "               ])\n",
    "\n",
    "model_stft = \\\n",
    "Pipeline(steps=[\n",
    "                ('ArraySTFT', tf.ArraySTFT()),\n",
    "                ('ArrayReshaper', tf.ArrayReshaper((3136))),\n",
    "                ('StandardScaler', StandardScaler()),\n",
    "                ('ArrayReshaper2', tf.ArrayReshaper((1, 56, 56))),\n",
    "                ('Model', ganomaly_stft)\n",
    "               ])\n",
    "\n",
    "model_fft = \\\n",
    "Pipeline(steps=[\n",
    "                ('ArrayFFT', tf.ArrayFFT()),\n",
    "                ('StandardScaler', StandardScaler()),\n",
    "                ('ArrayReshaper2', tf.ArrayReshaper((1, 1568))),\n",
    "                ('Model', ganomaly_fft)\n",
    "               ])\n",
    "\n",
    "\n",
    "model_timeseries.fit(train)\n",
    "model_stacked_ts.fit(train)\n",
    "model_fe.fit(train)\n",
    "model_stft.fit(train)\n",
    "model_fft.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 1568])\n",
      "torch.Size([64, 32, 195])\n",
      "torch.Size([64, 64, 48])\n",
      "torch.Size([64, 128, 9])\n",
      "torch.Size([64, 256, 1])\n",
      "torch.Size([64, 128, 9])\n",
      "torch.Size([64, 64, 49])\n",
      "torch.Size([64, 32, 196])\n",
      "torch.Size([64, 16, 1568])\n"
     ]
    }
   ],
   "source": [
    "inputx = torch.zeros((64, 16, 1568))\n",
    "print(inputx.shape)\n",
    "\n",
    "l0 = torch.nn.Conv1d(16, 32, 16, 8, padding = 3, bias = False)\n",
    "\n",
    "l1 = torch.nn.Conv1d(32, 64, 16, 4, padding = 6, bias = False)\n",
    "\n",
    "l2 = torch.nn.Conv1d(64, 128, 16, 4, padding = 0, bias = False)\n",
    "\n",
    "l3 = torch.nn.Conv1d(128, 256, 9, 4, padding = 0, bias = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr2 = torch.nn.ConvTranspose1d(256, 128, 9, 4, padding = 0, bias = False)\n",
    "\n",
    "lr3 = torch.nn.ConvTranspose1d(128, 64, 16, 4, padding = 0, output_padding=1, bias = False)\n",
    "\n",
    "lr4 = torch.nn.ConvTranspose1d(64, 32, 16, 4, padding = 6, bias = False)\n",
    "\n",
    "lr5 = torch.nn.ConvTranspose1d(32, 16, 16, 8, padding = 4, bias = False)\n",
    "\n",
    "\n",
    "o0 = l0(inputx)\n",
    "o1 = l1(o0)\n",
    "o2 = l2(o1)\n",
    "o3 = l3(o2)\n",
    "print(o0.shape)\n",
    "print(o1.shape)\n",
    "print(o2.shape)\n",
    "print(o3.shape)\n",
    "\n",
    "\n",
    "or2 = lr2(o3)\n",
    "or3 = lr3(or2)\n",
    "or4 = lr4(or3)\n",
    "or5 = lr5(or4)\n",
    "\n",
    "\n",
    "#print(or1.shape)\n",
    "print(or2.shape)\n",
    "print(or3.shape)\n",
    "print(or4.shape)\n",
    "print(or5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 1568])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size([64, 16, 1568])\n",
    "torch.Size([64, 32, 195])\n",
    "torch.Size([64, 64, 48])\n",
    "torch.Size([64, 128, 9])\n",
    "torch.Size([64, 256, 1])\n",
    "torch.Size([64, 128, 9])\n",
    "torch.Size([64, 64, 49])\n",
    "torch.Size([64, 32, 196])\n",
    "torch.Size([64, 16, 1568])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size([64, 64, 196])\n",
    "torch.Size([64, 128, 49])\n",
    "torch.Size([64, 256, 9])\n",
    "torch.Size([64, 512, 1])\n",
    "torch.Size([64, 256, 9])\n",
    "torch.Size([64, 128, 49])\n",
    "torch.Size([64, 64, 196])\n",
    "torch.Size([64, 32, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 27, 27])\n",
      "torch.Size([64, 64, 13, 13])\n",
      "torch.Size([64, 128, 6, 6])\n",
      "torch.Size([64, 256, 3, 3])\n",
      "torch.Size([64, 512, 1, 1])\n",
      "torch.Size([64, 256, 3, 3])\n",
      "torch.Size([64, 128, 7, 7])\n",
      "torch.Size([64, 64, 14, 14])\n",
      "torch.Size([64, 32, 28, 28])\n",
      "torch.Size([64, 16, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "inputx = torch.zeros((64, 16, 55, 55))\n",
    "\n",
    "l0 = torch.nn.Conv2d(16, 32, 4, 2, padding = 1, bias = False)\n",
    "\n",
    "l1 = torch.nn.Conv2d(32, 64, 4, 2, padding = 1, bias = False)\n",
    "\n",
    "l2 = torch.nn.Conv2d(64, 128, 4, 2, padding = 1, bias = False)\n",
    "\n",
    "l3 = torch.nn.Conv2d(128, 256, 4, 2, padding = 1, bias = False)\n",
    "\n",
    "l4 = torch.nn.Conv2d(256, 512, 3, 1, padding = 0, bias = False)\n",
    "\n",
    "o0 = l0(inputx)\n",
    "o1 = l1(o0)\n",
    "o2 = l2(o1)\n",
    "o3 = l3(o2)\n",
    "o4 = l4(o3)\n",
    "\n",
    "print(o0.shape)\n",
    "print(o1.shape)\n",
    "print(o2.shape)\n",
    "print(o3.shape)\n",
    "print(o4.shape)\n",
    "\n",
    "lr1 = torch.nn.ConvTranspose2d(512, 256, 3, 1, padding = 0, bias = False)\n",
    "\n",
    "lr2 = torch.nn.ConvTranspose2d(256, 128, 4, 2, padding = 1, output_padding = 1, bias = False)\n",
    "\n",
    "lr3 = torch.nn.ConvTranspose2d(128, 64, 4, 2, padding = 1, bias = False)\n",
    "\n",
    "lr4 = torch.nn.ConvTranspose2d(64, 32, 4, 2, padding = 1, bias = False)\n",
    "\n",
    "lr5 = torch.nn.ConvTranspose2d(32, 16, 4, 2, padding = 1, bias = False)\n",
    "\n",
    "or1 = lr1(o4)\n",
    "or2 = lr2(or1)\n",
    "or3 = lr3(or2)\n",
    "or4 = lr4(or3)\n",
    "or5 = lr5(or4)\n",
    "\n",
    "\n",
    "print(or1.shape)\n",
    "print(or2.shape)\n",
    "print(or3.shape)\n",
    "print(or4.shape)\n",
    "print(or5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-b07dbda9e9df>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-b07dbda9e9df>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    cngf=\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "main = nn.Sequential()\n",
    "\n",
    "nz = 100\n",
    "cngf=\n",
    "\n",
    "main.add_module('initial-{0}-{1}-convt'.format(nz, cngf),\n",
    "                nn.ConvTranspose1d(nz, cngf, input_size, stride=1, output_padding = output_paddings[0] % 2, bias = False))\n",
    "main.add_module('initial-{0}-relu'.format(cngf),\n",
    "                nn.ReLU(True))\n",
    "main.add_module('pyramid-{0}-batchnorm'.format(cngf),\n",
    "                nn.BatchNorm1d(cngf))\n",
    "\n",
    "for output_padding in output_paddings[1:-1]:\n",
    "    main.add_module('pyramid-{0}-{1}-convt'.format(cngf, cngf // 2),\n",
    "                    nn.ConvTranspose1d(cngf, cngf // 2, 16, stride=4, output_padding = output_padding, bias = False))\n",
    "    main.add_module('pyramid-{0}-relu'.format(cngf // 2),\n",
    "                    nn.ReLU(True))\n",
    "    main.add_module('pyramid-{0}-batchnorm'.format(cngf // 2),\n",
    "                    nn.BatchNorm1d(cngf // 2))\n",
    "\n",
    "    cngf = cngf // 2\n",
    "\n",
    "\n",
    "main.add_module('final-{0}-{1}-convt'.format(cngf, nc),\n",
    "                    nn.ConvTranspose1d(cngf, nc, 16, stride=4, output_padding = output_paddings[-1], bias = False))\n",
    "main.add_module('final-{0}-tanh'.format(nc),\n",
    "                    nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1568\n",
    "\n",
    "\n",
    "while input_size >= 46:\n",
    "    input_size = input_size // 4\n",
    "    \n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputx = torch.zeros((64, 128, 49, 4))\n",
    "\n",
    "x = torch.nn.Conv2d(128, 64, 4, stride=1, bias = False)\n",
    "y = torch.nn.ConvTranspose2d(64, 128, 4, stride=1, bias = False)\n",
    "print(x(inputx).shape)\n",
    "y(x(inputx)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3136\n",
    "\n",
    "output_paddings = []\n",
    "\n",
    "n_layers = 0\n",
    "\n",
    "while input_size >= 46:\n",
    "    n_layers += 1\n",
    "    output_paddings.append(input_size % 4)\n",
    "    print(input_size)\n",
    "    print(input_size % 4)\n",
    "    input_size = input_size // 4\n",
    "\n",
    "input_size // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1568 // 4 // 4 // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1568 / 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "36 * 40 * 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1568 / 4 / 4 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3136 / 4 / 4 / 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(array) for array in [[1], [2,3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft(common_preprocessing.transform(cwruData)[:1], nperseg=112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test-split of the normal CWRU data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3136 // 4 // 4 // 4 // 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_parameters = {\n",
    "    'module_w_fra': list(range(0, 101, 10)),\n",
    "    'module_w_app': list(range(0, 101, 10)),\n",
    "    'module_w_lat': list(range(0, 101, 10)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ganomaly_gs = GridSearchCV(ganomaly, search_parameters, refit=False, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ganomaly_gs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization (best parameters)\n",
    "Adding a TensorBoard for the visualization of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganomaly_board = GANomalyBoard(SummaryWriter(), key_mapper = rename_tensorboard_key, close_after_train = False)\n",
    "ganomaly_fe.callbacks += [ganomaly_board]\n",
    "\n",
    "ganomaly_fe.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganomaly_fe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganomaly_fe.predict(test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganomaly_fe.predict(selection_pipeline.set_params(DataSelector__column_values = {'condition': ['Outer Race Fault'], 'sampleRate': [12000]}).transform(cwruData)[:400]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "56 * 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(1568)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3136 // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3136 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1568 / 4 / 4 / 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
