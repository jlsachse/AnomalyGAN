{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import lib.transformers as tfs\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from lib.models import Ganomaly1d, Ganomaly2d, GanomalyFE, GanomalyNet\n",
    "from lib.visualization import GANomalyBoard, rename_tensorboard_key\n",
    "\n",
    "from skorch.callbacks import PassthroughScoring, ProgressBar\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import tensorflow\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from lib.others import create_dataset\n",
    "\n",
    "from lib.others import build_model\n",
    "from lib.visualization import lineplot_comparison\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwruData0 = pd.read_parquet('data/cwru0.parquet')\n",
    "cwruData1 = pd.read_parquet('data/cwru1.parquet')\n",
    "\n",
    "cwruData = pd.concat([cwruData0, cwruData1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_features, normal_labels = create_dataset(cwruData, feature_columns = ['fanEndData', 'driveEndData'], label_columns = ['condition', 'faultDiameter', 'motorLoad', 'relativeFaultPosition', 'faultyBearingPosition'], condition = ['Normal Baseline'], sampleRate = [12000])\n",
    "normal_features_train, normal_features_test, normal_labels_train, normal_labels_test = train_test_split(normal_features, normal_labels, train_size = 400, test_size= 451)\n",
    "\n",
    "ball_features, ball_labels = create_dataset(cwruData, feature_columns = ['fanEndData'], label_columns = ['condition', 'faultDiameter', 'motorLoad', 'relativeFaultPosition', 'faultyBearingPosition'], condition = ['Ball Fault'], sampleRate = [12000])\n",
    "inner_features, inner_labels = create_dataset(cwruData, feature_columns = ['fanEndData'], label_columns = ['condition', 'faultDiameter', 'motorLoad', 'relativeFaultPosition', 'faultyBearingPosition'], condition = ['Inner Race Fault'], sampleRate = [12000])\n",
    "outer_features, outer_labels = create_dataset(cwruData, feature_columns = ['fanEndData'], label_columns = ['condition', 'faultDiameter', 'motorLoad', 'relativeFaultPosition', 'faultyBearingPosition'], condition = ['Outer Race Fault'], sampleRate = [12000])\n",
    "\n",
    "\n",
    "labels_test = pd.concat([ball_labels, inner_labels, outer_labels, normal_labels_test])\n",
    "features_test = pd.concat([ball_features, inner_features, outer_features, normal_features_test])\n",
    "\n",
    "normal_features_test = np.array(normal_features_test.to_list())\n",
    "normal_features_train = np.array(normal_features_train.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the time series model and optimizing it it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "\n",
    "n_z = 600\n",
    "n_channels = 1\n",
    "n_feature_maps = 64\n",
    "\n",
    "adversarial_weight = 1\n",
    "contextual_weight = 1\n",
    "encoder_weight = 1\n",
    "lambda_weight = 0.5\n",
    "\n",
    "# training settings\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else 'cpu'\n",
    "n_gpus = 0\n",
    "workers = 2\n",
    "batch_size = 16\n",
    "max_epochs = 50\n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# extra callbacks\n",
    "callbacks = []\n",
    "\n",
    "# run number\n",
    "run = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialization\n",
    "\n",
    "gan_time_series =\\\n",
    "build_model(\n",
    "    model = Ganomaly1d,\n",
    "    \n",
    "    # model parameters\n",
    "    input_size = 3136, \n",
    "    n_z = n_z,\n",
    "    n_channels = n_channels,\n",
    "    n_fm_discriminator = n_feature_maps,  \n",
    "    n_fm_generator = n_feature_maps,\n",
    "    adversarial_weight = adversarial_weight,\n",
    "    contextual_weight = contextual_weight, \n",
    "    encoder_weight = encoder_weight,\n",
    "    lambda_weight = lambda_weight,\n",
    "    \n",
    "    # training parameters\n",
    "    device = device,\n",
    "    n_gpus = n_gpus,\n",
    "    workers = workers,\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = max_epochs, \n",
    "    lr = 0.0001,\n",
    "    beta1 = 0.5,\n",
    "    beta2 = 0.999, \n",
    "    \n",
    "    # extra callbacks\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_time_series = \\\n",
    "Pipeline(steps=[\n",
    "                ('reshaper', tfs.ArrayReshaper((1, 3136))),\n",
    "                ('retyper', tfs.ArrayRetyper(np.float32)),\n",
    "                ('model', gan_time_series)\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search search space\n",
    "\n",
    "search_space = [0, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "search_parameters = \\\n",
    "[\n",
    "    {\n",
    "    'model__module__adversarial_weight': search_space,\n",
    "    },\n",
    "    {\n",
    "    'model__module__contextual_weight': search_space,\n",
    "    },\n",
    "    {\n",
    "    'model__module__encoder_weight': search_space,\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganomaly_gs = GridSearchCV(pipeline_time_series, search_parameters, refit=False, cv=4, verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganomaly_gs.fit(normal_features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12, 6)}, style = 'darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for iteration in range(4):\n",
    "    \n",
    "    param_df = pd.DataFrame([[list(entry.keys())[0], list(entry.values())[0]] for entry in ganomaly_gs.cv_results_['params']]).rename({0: 'hyperparameter', 1: 'value'}, axis = 1)\n",
    "    results_df = pd.DataFrame([abs(ganomaly_gs.cv_results_[f'split{iteration}_test_generator_loss']), abs(ganomaly_gs.cv_results_[f'split{iteration}_test_train_loss'])]).T.rename({0: 'generator_loss', 1: 'train_loss'}, axis = 1)\n",
    "\n",
    "    results_df = pd.concat([param_df, results_df], axis = 1)\n",
    "    results_df['hyperparameter'] = results_df['hyperparameter'].str.replace('model__module__', '')\n",
    "    \n",
    "    \n",
    "    if iteration > 0:\n",
    "        results_df = pd.concat([last_df, results_df])\n",
    "    \n",
    "    last_df = results_df\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_figure, train_loss_ax = plt.subplots()\n",
    "train_loss_ax = sns.lineplot(data = results_df, x = 'value', y = 'train_loss', hue = 'hyperparameter', err_style='bars', ax = train_loss_ax)\n",
    "weight = results_df.groupby(['hyperparameter', 'value']).mean().idxmin()['train_loss'][1]\n",
    "\n",
    "train_loss_ax.axvline(weight, ls='--', color = 'grey')\n",
    "train_loss_ax.set_ylabel('Train Loss')\n",
    "train_loss_ax.set_xlabel('Weight')\n",
    "train_loss_ax.set_title('Train Loss in Relation to Weights')\n",
    "\n",
    "legend = train_loss_ax.legend_\n",
    "legend.set_title('Hyperparameter')\n",
    "for text in legend.texts:\n",
    "    text.set_text(text.get_text().title().replace('_', ' '))\n",
    "    \n",
    "train_loss_figure.savefig('data/results/hyperparameter-tuning_train-loss.png', dpi=330, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_loss_figure, generator_loss_ax = plt.subplots()\n",
    "generator_loss_ax = sns.lineplot(data = results_df, x = 'value', y = 'generator_loss', hue = 'hyperparameter', err_style='bars', ax = generator_loss_ax)\n",
    "weight = results_df.groupby(['hyperparameter', 'value']).mean().idxmin()['generator_loss'][1]\n",
    "\n",
    "generator_loss_ax.axvline(weight, ls='--', color = 'grey')\n",
    "generator_loss_ax.set_ylabel('Generator Loss')\n",
    "generator_loss_ax.set_xlabel('Weight')\n",
    "generator_loss_ax.set_title('Generator Loss in Relation to Weights')\n",
    "\n",
    "legend = generator_loss_ax.legend_\n",
    "legend.set_title('Hyperparameter')\n",
    "for text in legend.texts:\n",
    "    text.set_text(text.get_text().title().replace('_', ' '))\n",
    "    \n",
    "generator_loss_figure.savefig('data/results/hyperparameter-tuning_generator-loss.png', dpi=330, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
